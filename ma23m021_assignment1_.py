# -*- coding: utf-8 -*-
"""ma23m021_Assignment1_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10QZ_QiLJq96RdyrdoyJlVyV-O39j1uI2
"""

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import fashion_mnist
from sklearn.model_selection import train_test_split

import wandb
import random
from types import SimpleNamespace
#wandb.finish()
wandb.login(key = "MY_API_KEY")

wandb.login()
wandb.init(project="MA23M021_A1", name="One_Image_Per_Class", reinit=True)

# Activation functions................
def relu(x):
    return np.maximum(0, x)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

# Define activation function derivatives.........................
def relu_derivative(x):
    return np.where(x > 0, 1, 0)

def sigmoid_derivative(x):
    sig = 1 / (1 + np.exp(-x))
    return sig * (1 - sig)

def tanh_derivative(x):
    return 1 - np.tanh(x) ** 2

# initializing weights and biases.......................................
def param_init(input_size, num_hidden_layers, num_neurons, output_size=10, init_method='xavier'):
    layers = [input_size] + [num_neurons] * num_hidden_layers + [output_size]
    weights = {}
    biases = {}

    # initialization choices.......................................
    for i in range(1, len(layers)):
        if init_method == 'xavier':
            weights[f'W{i}'] = np.random.randn(layers[i - 1], layers[i]) * np.sqrt(2 / layers[i - 1])
        elif init_method == 'random':
            weights[f'W{i}'] = np.random.randn(layers[i - 1], layers[i]) * 0.01
        else:
            raise ValueError("Invalid init_method! Use 'xavier' or 'random'.")

        biases[f'b{i}'] = np.zeros((1, layers[i]))

    return weights, biases

# forward propagation........................................
def forwardd_propagation_(x, num_hidden_layers, num_neurons, weights=None, biases=None, output_size=10, activation_func='sigmoid'):
    input_size = x.shape[1]
    #print("I am in forward prop : ")
    # initializing the parameters that is weights and bias.........................
    if weights is None or biases is None:
        weights, biases = param_init(input_size, num_hidden_layers, num_neurons, output_size)

    activations = {0: x}
    z_values = {}
# updation loop.......................................
    for i in range(1, len(weights) + 1):
        z_values[i] = np.dot(activations[i - 1], weights[f'W{i}']) + biases[f'b{i}']
        if i < len(weights):
            if activation_func == 'relu':
                activations[i] = relu(z_values[i])
            elif activation_func == 'tanh':
                activations[i] = tanh(z_values[i])
            else:
                activations[i] = sigmoid(z_values[i])
        else:
            activations[i] = softmax(z_values[i])

    return activations[len(weights)], activations, z_values, weights, biases


# Updated backward propagation....................................
def backwardd_propagation_(y, activations, z_values, weights, activation_function):
    grads = {}
    m = y.shape[0]
    #print("I am in backward prop : ")
    def get_derivative(activation_func):
        if activation_func == 'relu':
            return relu_derivative
        elif activation_func == 'sigmoid':
            return sigmoid_derivative
        elif activation_func == 'tanh':
            return tanh_derivative

    hidden_derivative = get_derivative(activation_function)

    dz = activations[len(weights)] - y

    # Gradient calculation loop.................................
    for i in reversed(range(1, len(weights) + 1)):
        grads[f"dW{i}"] = np.dot(activations[i-1].T, dz) / m
        grads[f"db{i}"] = np.sum(dz, axis=0, keepdims=True) / m
        if i > 1:
            dz = np.dot(dz, weights[f'W{i}'].T) * hidden_derivative(z_values[i-1])

    return grads

# fashion-MNIST dataset...............................
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values.........................
X_train = X_train.astype(np.float32) / 255.0
X_test = X_test.astype(np.float32) / 255.0

# flattening images.................................
X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)

# def onehot_encoding(y, num_classes=10):
#     return np.eye(num_classes = 10)[y]

# One-hot encode labels.........................
#y_train = onehot_encoding(y_train)
#y_test = onehot_encoding(y_test)


# Split data.................................
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)



# function for stochastic gradient optimizer .........................
def sgd(weights, biases, gradients, params):
    lr = params.get("learningg_rate_", 0.1)
    print("You are in SGD")
    for i in range(1, len(weights) + 1):
        
        weights[f"W{i}"] -= lr * gradients[f"dW{i}"]
        biases[f"b{i}"] -= lr * gradients[f"db{i}"]
    
    return weights, biases



# function for Adam optimizer ..............................
def adam(weights, biases, gradients, params):
    lr, beta1, beta2, epsilon = params.get("learningg_rate_", 0.1), 0.9, 0.999, 1e-8
    #print("learning rate in ADAM :",lr)
    # Initializing momentum and velocity dictionaries.............................
    m_w = params.setdefault("m_w", {f"W{i}": np.zeros_like(weights[f"W{i}"]) for i in range(1, len(weights) + 1)})
    v_w = params.setdefault("v_w", {f"W{i}": np.zeros_like(weights[f"W{i}"]) for i in range(1, len(weights) + 1)})
    m_b = params.setdefault("m_b", {f"b{i}": np.zeros_like(biases[f"b{i}"]) for i in range(1, len(biases) + 1)})
    v_b = params.setdefault("v_b", {f"b{i}": np.zeros_like(biases[f"b{i}"]) for i in range(1, len(biases) + 1)})
    t = params.setdefault("t", 0) + 1
    #print("You are in ADAM")
    for i in range(1, len(weights) + 1):
        w_key, b_key = f"W{i}", f"b{i}"

        m_w[w_key] = beta1 * m_w[w_key] + (1 - beta1) * gradients[f'd{w_key}']
        v_w[w_key] = beta2 * v_w[w_key] + (1 - beta2) * (gradients[f'd{w_key}'] ** 2)
        m_b[b_key] = beta1 * m_b[b_key] + (1 - beta1) * gradients[f'd{b_key}']
        v_b[b_key] = beta2 * v_b[b_key] + (1 - beta2) * (gradients[f'd{b_key}'] ** 2)

        m_w_hat, v_w_hat = m_w[w_key] / (1 - beta1**t), v_w[w_key] / (1 - beta2**t)
        m_b_hat, v_b_hat = m_b[b_key] / (1 - beta1**t), v_b[b_key] / (1 - beta2**t)

        # Updateing  weights and biases...............................
        weights[w_key] -= lr * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
        biases[b_key] -= lr * m_b_hat / (np.sqrt(v_b_hat) + epsilon)

    params["t"] = t  # Update timestep
    return weights, biases

#function for momentum optimizer ..............................
def momentum(weights, biases, gradients, params):
    lr, beta = params.get("learningg_rate_", 0.01), params.get("beta", 0.9)
    v_w = params.setdefault("v_w", {f"W{i}": np.zeros_like(weights[f"W{i}"]) for i in range(1, len(weights) + 1)})
    v_b = params.setdefault("v_b", {f"b{i}": np.zeros_like(biases[f"b{i}"]) for i in range(1, len(biases) + 1)})
    for i in range(1, len(weights) + 1):
        w_key_, b_key_ = f"W{i}", f"b{i}"
        v_w[w_key_] = beta * v_w[w_key_] + (1 - beta) * gradients[f"dW{i}"]
        v_b[b_key_] = beta * v_b[b_key_] + (1 - beta) * gradients[f"db{i}"]
        weights[w_key_] -= lr * v_w[w_key_]
        biases[b_key_] -= lr * v_b[b_key_]
    return weights, biases


# function for neaterov optimizer ..............................
def nesterov(weights, biases, gradients, params):
    lr, mu = params.get("learningg_rate_", 0.01), 0.9
    v_w = params.setdefault("v_w", {f"W{i}": np.zeros_like(weights[f"W{i}"]) for i in range(1, len(weights) + 1)})
    v_b = params.setdefault("v_b", {f"b{i}": np.zeros_like(biases[f"b{i}"]) for i in range(1, len(biases) + 1)})

    for i in range(1, len(weights) + 1):
        v_w[f"W{i}"] = mu * v_w[f"W{i}"] + lr * gradients[f"dW{i}"]
        v_b[f"b{i}"] = mu * v_b[f"b{i}"] + lr * gradients[f"db{i}"]
        weights[f"W{i}"] -= v_w[f"W{i}"]   # updating weights
        biases[f"b{i}"] -= v_b[f"b{i}"]
    return weights, biases

#function for rmsprop optimizer ..............................
def rmsprop(weights, biases, gradients, params):
    lr, beta, epsilon = params.get("learningg_rate_", 0.001), 0.9, 1e-8
    s_w = params.setdefault("s_w", {f"W{i}": np.zeros_like(weights[f"W{i}"]) for i in range(1, len(weights) + 1)})
    s_b = params.setdefault("s_b", {f"b{i}": np.zeros_like(biases[f"b{i}"]) for i in range(1, len(biases) + 1)})

    for i in range(1, len(weights) + 1):
        s_w[f"W{i}"] = beta * s_w[f"W{i}"] + (1 - beta) * gradients[f"dW{i}"]**2
        s_b[f"b{i}"] = beta * s_b[f"b{i}"] + (1 - beta) * gradients[f"db{i}"]**2
        weights[f"W{i}"] -= lr * gradients[f"dW{i}"] / (np.sqrt(s_w[f"W{i}"]) + epsilon)
        biases[f"b{i}"] -= lr * gradients[f"db{i}"] / (np.sqrt(s_b[f"b{i}"]) + epsilon)
    return weights, biases

#function for nadam optimizer ..............................
def nadam(weights, biases, gradients, params):
    lr, beta1, beta2, epsilon = params.get("learningg_rate_", 0.001), 0.9, 0.999, 1e-8

    # Initialize momentum and velocity dictionaries...............................
    m_w = params.setdefault("m_w", {f"W{i}": np.zeros_like(weights[f"W{i}"]) for i in range(1, len(weights) + 1)})
    v_w = params.setdefault("v_w", {f"W{i}": np.zeros_like(weights[f"W{i}"]) for i in range(1, len(weights) + 1)})
    m_b = params.setdefault("m_b", {f"b{i}": np.zeros_like(biases[f"b{i}"]) for i in range(1, len(biases) + 1)})
    v_b = params.setdefault("v_b", {f"b{i}": np.zeros_like(biases[f"b{i}"]) for i in range(1, len(biases) + 1)})
    t = params.setdefault("t", 0) + 1

    for i in range(1, len(weights) + 1):
        w_key, b_key = f"W{i}", f"b{i}"

        # Updating biased first moment estimate-momentum part............................
        m_w[w_key] = beta1 * m_w[w_key] + (1 - beta1) * gradients[f"dW{i}"]
        m_b[b_key] = beta1 * m_b[b_key] + (1 - beta1) * gradients[f"db{i}"]

        # Updating biased second moment estimate-RMSProp part...............................
        v_w[w_key] = beta2 * v_w[w_key] + (1 - beta2) * (gradients[f"dW{i}"] ** 2)
        v_b[b_key] = beta2 * v_b[b_key] + (1 - beta2) * (gradients[f"db{i}"] ** 2)

        # estimates of Bias-corrected first and second moments............................
        m_w_hat = m_w[w_key] / (1 - beta1**t)
        v_w_hat = v_w[w_key] / (1 - beta2**t)
        m_b_hat = m_b[b_key] / (1 - beta1**t)
        v_b_hat = v_b[b_key] / (1 - beta2**t)

        # Nesterov momentum:............................
        m_w_nesterov = beta1 * m_w_hat + (1 - beta1) * gradients[f"dW{i}"]
        m_b_nesterov = beta1 * m_b_hat + (1 - beta1) * gradients[f"db{i}"]

        # Updating weights and biases using Nadam...................
        weights[w_key] -= lr * m_w_nesterov / (np.sqrt(v_w_hat) + epsilon)
        biases[b_key] -= lr * m_b_nesterov / (np.sqrt(v_b_hat) + epsilon)

    params["t"] = t
    return weights, biases


# train function...................................
def trainn(X_train, Y_train, X_test, Y_test, num_hidden_layers=3, num_neurons=128, num_epochs=20, batch_size=32,
          learningg_rate_=0.01, optimizer_func=adam, init_method='xavier', activation_func='relu'):
    wandb.init(project="MA23M021_A1")
    input_size = X_train.shape[1]
    output_size = 10
    weights, biases = param_init(input_size, num_hidden_layers, num_neurons, output_size, init_method)
    optimizer_params = {"learningg_rate_": learningg_rate_}

    num_samples = X_train.shape[0]
    num_batches = num_samples // batch_size

    for epoch in range(num_epochs):
        perm = np.random.permutation(num_samples)
        X_train, Y_train = X_train[perm], Y_train[perm]

        for i in range(num_batches):
            batch_X = X_train[i * batch_size:(i + 1) * batch_size]
            batch_Y = Y_train[i * batch_size:(i + 1) * batch_size]

            # Forward propagation........................
            y_hat, activations, z_values, weights, biases = forwardd_propagation_(
                batch_X, num_hidden_layers, num_neurons, weights, biases, activation_func=activation_func
            )

            # Compute loss................................
            loss = -np.mean(np.sum(batch_Y * np.log(y_hat + 1e-8), axis=1))

            # Backward propagation........................
            grads = backwardd_propagation_(batch_Y, activations, z_values, weights, activation_func)

            # Update weights and biases...................
            weights, biases = optimizer_func(weights, biases, grads, optimizer_params)

        # Evaluate on test set after each epoch..........
        y_test_hat, _, _, _, _ = forwardd_propagation_(
            X_test, num_hidden_layers, num_neurons, weights, biases, activation_func=activation_func
        )
        predictions = np.argmax(y_test_hat, axis=1)
        accuracy = np.mean(predictions == np.argmax(Y_test, axis=1))

        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}")
        wandb.log({"loss": loss, "accuracy": accuracy})
    print(weights['W1'])
    return weights, biases





